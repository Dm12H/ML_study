{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, SPARK_HOME + \"/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Add the py4j to the path.\n",
    "# You may need to change the version number to match your install\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python/lib/py4j-0.10.3-src.zip'))\n",
    "\n",
    "# Initialize PySpark to predefine the SparkContext variable 'sc'\n",
    "execfile(os.path.join(SPARK_HOME, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a standard functions to load dataset and import all necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "wget -q -nc https://raw.githubusercontent.com/amitgroup/amitgroup/master/amitgroup/io/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: t10k-images-idx3-ubyte already exists;\tnot overwritten\n",
      "gzip: t10k-labels-idx1-ubyte already exists;\tnot overwritten\n",
      "gzip: train-images-idx3-ubyte already exists;\tnot overwritten\n",
      "gzip: train-labels-idx1-ubyte already exists;\tnot overwritten\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir -p mnist && {\n",
    "    cd mnist;\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz &&\n",
    "    gunzip *.gz\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = mnist.load_mnist(dataset='training', path='mnist/')\n",
    "X = X.reshape(-1, 1, 28, 28)\n",
    "\n",
    "X_test, y_test = mnist.load_mnist(dataset='testing', path='mnist/')\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating RDD for train and test, specifying number of partitions to be 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_rdd = sc.parallelize(\n",
    "    (i, y[i], X[i].ravel().copy())\n",
    "    for i in xrange(X.shape[0])\n",
    ").repartition(64).persist()\n",
    "\n",
    "X_test_rdd = sc.parallelize(\n",
    "    (i, y_test[i], X_test[i].ravel().copy())\n",
    "    for i in xrange(X_test.shape[0])\n",
    ").repartition(64).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking number of partitions\n",
    "X_train_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the base code from the seminar and built upon it. Here in epoch we state the update rule for weights and calculate weights themselves. The gradient is obtained from formula: \n",
    "$$\\omega_i^{t+1} = \\omega_i^t + \\nu\\frac{1}{N}\\sum_i^N \\sum_j x_i^j[y_i^j-\\hat{P}(Y_i^j=k \\>|\\> w,b)]$$, $y_i^j$ is an indicator of class j of observation i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, C = 1.0e-3):\n",
    "        self.W = np.random.uniform(-1, 1, size=(28 * 28, 10))\n",
    "        self.b = np.random.uniform(-1, 1, size=(10, ))\n",
    "        self.I = np.identity(10)\n",
    "        self.size = 60000\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        ### note that this is not an efficient implementation\n",
    "        ### of X.dot(W)\n",
    "        exps = np.exp(x.dot(self.W)+self.b)\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def epoch(self,x,learning_rate, bch):\n",
    "        N = 1.0/(self.size*bch)\n",
    "        batch = x.sample(False,bch).cache()\n",
    "        W_grad = batch.map(lambda (i, y, x): N*np.outer(x,self.I[y]-self.softmax(x))).reduce(lambda a,b: a+b)\n",
    "        b_grad = batch.map(lambda (i, y, x): N*(self.I[y]-self.softmax(x))).reduce(lambda a,b: a+b)\n",
    "        self.W = self.W + learning_rate*W_grad\n",
    "        self.b = self.b + learning_rate*b_grad\n",
    "        \n",
    "    def train(self,x,lr=1, n_epochs = 100,aim=0.80,extension =5,significance = 0.01):\n",
    "        best_acc = 0\n",
    "        timer = extension+1\n",
    "        for i in range(n_epochs):\n",
    "            self.epoch(x,lr,0.5)\n",
    "            y_labels = x.map(lambda (i, y, x): y).collect()\n",
    "            y_predict = x.map(lambda (i, y, x): np.argmax(self.softmax(x))).collect()\n",
    "            accuracy = accuracy_score(y_labels,y_predict)\n",
    "            if accuracy > best_acc+significance:\n",
    "                best_acc = accuracy\n",
    "                timer = extension+1\n",
    "            if accuracy > aim:\n",
    "                timer -=1\n",
    "                if timer == 0:\n",
    "                    print \"required accuracy reached\"\n",
    "                    print 'best accuracy after %d epochs  : ' %i, best_acc\n",
    "                    break\n",
    "                \n",
    "    def test_pred(self, x):\n",
    "        y_t = x.map(lambda (i, y, x): y).collect()\n",
    "        y_t_predict = x.map(lambda (i, y, x): np.argmax(self.softmax(x))).collect()\n",
    "        accuracy = accuracy_score(y_t,y_t_predict)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes forever to reach 85%, as accuracy increases steadily but increasingly slow, so i decided to cut it at 80%. Due to big sample size, overfitting is not the case, as seen from check on test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required accuracy reached\n",
      "best accuracy after 53 epochs  :  0.803166666667\n"
     ]
    }
   ],
   "source": [
    "L = LogisticRegression()\n",
    "L.train(X_train_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81569999999999998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.test_pred(X_test_rdd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
